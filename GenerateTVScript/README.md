# Generate TV Script with RNN
In this project, we generate Seinfeld TV scripts using RNNs. The model learns from Seinfeld dataset and generates fake TV script.

## Data
We use part of the [Seinfeld dataset](https://www.kaggle.com/thec03u5/seinfeld-chronicles#scripts.csv) of scripts from 9 seasons. The first several lines of training data looks like below:

<i>
jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. 

jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. 

george: are you through? 

jerry: you do of course try on, when you buy? 

george: yes, it was purple, i liked it, i dont actually recall considering the buttons. 
</i>

## Project Steps
1. Pre-process data - tokenize, and create a dictionary of word to integer
2. Build network - there is embedding layer and LSTM layer. The size of embedding dimension, number of LSTM layers, number of hidden units in LSTM layer, as well as length of output sequence are the hyperparameters of the model.
3. Train - aim at los lower than <b>3.5</b>
4. Result - generate scripts from the trained model and see how real it looks like.

## Hyperparamter tuning
The Initially, I was trying a smaller set of hidden dimension (16, 32, 64, 128) with a few number of layers (2, 3). But it did not converge well and the loss stopped around 4. I increased the hidden_dimension to a larger number (256) and it started to have a better result. I've also adjusted the learning rate to be smaller as the higher learning rate (0.01 or 0.005) did not have good convergence property. I've also tried shorter sequence length (5,10) but did not converge well either.

## Result

The fake script generated by the RNN looks like below:

<i>
jerry: this is excellent, we're gonna be in there.

hoyt: so i guess you can go to the stand.

george: well, i guess i can be able to call a plane.

jerry: what?

george: no.

jerry: so you don't think that would make the plane?

george: i don't know. i don't know if you could be able to help you.

jerry: i can't do it.

jerry: what?

george: what?

elaine: i was a kid.

jerry: so what do you think?

george: what?

george: i think it was a lot of water.

hoyt: so, what are you gonna do?

jerry: i know.

george: what?

jerry: what is that?

jerry: i don't know, i didn't tell you.

hoyt: call him to a doctor and i want to see the truth.

kramer: oh yeah.

elaine: you know, i don't want to get a car accident.

elaine: oh, yeah.

jerry: so what do i want to do with you?

jerry: well, it's not that... i think it would be a good time.

george: i know.

george: what is it?

elaine: you know what? i think i can be able to get the car out of there.

george: i know, i know, i know. it's not a good time. i mean, we were just trying to get out of my mind.

elaine: oh, yeah, but i don't think so.

jerry: i know, he was a good samaritan- one.

[new witness: police call.

elaine: what? what is this?

jerry: yeah, i know.

chiles: you know what? i think it was a good idea of honor.

hoyt: so we can see how you would have a lot of coffee
</i>
